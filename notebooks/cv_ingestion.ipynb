{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1dd9d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\Desktop\\RAG_PROJECT\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Hp\\Desktop\\RAG_PROJECT\\venv\\Lib\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from uuid import uuid4\n",
    "import pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2935ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81017a4",
   "metadata": {},
   "source": [
    "### Initialize Pinecone and Create and Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e126aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone index: cv2-index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Load .env\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "if api_key is None:\n",
    "    raise ValueError(\"Missing Pinecone API key. Did you set it in your .env?\")\n",
    "\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "index_name = \"cv2-index\"\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,  # Match SentenceTransformer embedding size\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    #pc.describe_index(index_name).wait_until_ready()\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n",
    "print(f\"Connected to Pinecone index: {index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89ac79",
   "metadata": {},
   "source": [
    "### Load and Chunk Your CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c48c6d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\Desktop\\RAG_PROJECT\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83129768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total chunks created: 5\n",
      "üìÇ Saved to ../data/csv/cv_chunks.json\n"
     ]
    }
   ],
   "source": [
    "# src/components/cv_ingestion_pdf.py\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "\n",
    "def load_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"{pdf_path} not found\")\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def split_cv_by_sections(text: str) -> Dict[str, str]:\n",
    "    \"\"\"Split CV text into sections using headings.\"\"\"\n",
    "    sections = [\"Summary\", \"Skills\", \"Experience\", \"Education\", \"Certifications\"]\n",
    "    pattern = \"(\" + \"|\".join(sections) + \")\"\n",
    "    \n",
    "    matches = list(re.finditer(pattern, text, re.IGNORECASE))\n",
    "    section_dict = {}\n",
    "    \n",
    "    for i, match in enumerate(matches):\n",
    "        start = match.end()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(text)\n",
    "        section_name = match.group().title()\n",
    "        section_text = text[start:end].strip()\n",
    "        section_dict[section_name] = section_text\n",
    "    \n",
    "    return section_dict\n",
    "\n",
    "\n",
    "def chunk_section_text(section_text: str, chunk_size: int = 300, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Split section text into word-based chunks (optional overlap).\"\"\"\n",
    "    words = section_text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_cv_sections(cv_text: str) -> List[Dict]:\n",
    "    \"\"\"Split CV into section-based chunks with metadata.\"\"\"\n",
    "    sections = split_cv_by_sections(cv_text)\n",
    "    chunks_list = []\n",
    "\n",
    "    for section_name, section_text in sections.items():\n",
    "        section_chunks = chunk_section_text(section_text)\n",
    "        for i, chunk in enumerate(section_chunks):\n",
    "            chunks_list.append({\n",
    "                \"id\": f\"{section_name}_{i}\",\n",
    "                \"section\": section_name,\n",
    "                \"chunk_index\": i,\n",
    "                \"content\": chunk\n",
    "            })\n",
    "    \n",
    "    return chunks_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = \"../data/csv/resume_project.pdf\"\n",
    "    output_path = \"../data/csv/cv_chunks.json\"\n",
    "\n",
    "    # 1. Load PDF text\n",
    "    cv_text = load_pdf_text(pdf_path)\n",
    "\n",
    "    # 2. Chunk by sections\n",
    "    chunks = chunk_cv_sections(cv_text)\n",
    "\n",
    "    # 3. Save chunks to JSON\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Total chunks created: {len(chunks)}\")\n",
    "    print(f\"üìÇ Saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff265da1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c38e5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence Transformer model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ed7e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/pipeline/cv_embeddings.py\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "def embed_cv_chunks(input_json=\"data/cv_chunks.json\", output_json=\"data/cv_embeddings.json\"):\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunks = json.load(f)\n",
    "\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        emb = model.encode(chunk[\"content\"]).tolist()\n",
    "        embeddings.append({\n",
    "            \"id\": chunk[\"id\"],\n",
    "            \"section\": chunk[\"section\"],\n",
    "            \"content\": chunk[\"content\"],\n",
    "            \"embedding\": emb\n",
    "        })\n",
    "\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(embeddings, f, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(embeddings)} CV embeddings to {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2466e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 CV embeddings to ../data/csv/cv_embeddings.json\n"
     ]
    }
   ],
   "source": [
    "embed_cv_chunks(input_json=\"../data/csv/cv_chunks.json\", output_json=\"../data/csv/cv_embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49572123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 5 CV chunks into cv2-index\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/csv/cv_embeddings.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "to_upsert = [(d[\"id\"], d[\"embedding\"], {\"section\": d[\"section\"], \"content\": d[\"content\"]}) for d in data]\n",
    "index.upsert(vectors=to_upsert)\n",
    "\n",
    "print(f\"Upserted {len(to_upsert)} CV chunks into {index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31b3cc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query Results:\n",
      "Score: 0.501\n",
      "Section: Soft Skills: Analytical Thinking, Problem-Solving, Communication, Self-Directed Learning \n",
      "Experience      \n",
      " \n",
      "Zindi Africa ‚Äì Data Science Competitions April 2025 ‚Äì Present \n",
      "ÔÇ∑ Participated in multiple machine learning competitions tackling real-world problems in healthcare and agriculture.\n",
      "Content: Soft Skills: Analytical Thinking, Problem-Solving, Communication, Self-Directed Learning \n",
      "Experience      \n",
      " \n",
      "Zindi Africa ‚Äì Data Science Competitions April 2025 ‚Äì Present \n",
      "ÔÇ∑ Participated in multiple m...\n",
      "\n",
      "Score: 0.492\n",
      "Section: data into actionable insights and delivering production-ready ML solutions. Competitive ML participant (top \n",
      "30% Zindi), passionate about solving high-impact problems with data. \n",
      "Skills      \n",
      "Programming & Analytics: Python (pandas, NumPy, scikit-learn, matplotlib, seaborn), SQL, Excel \n",
      "Machine Learning & AI: Classification, Regression, NLP, Deep Learning, Feature Engineering, Model Evaluation \n",
      "LLMs & Conversational AI: LangChain, RAG, Vector Databases (Pinecone, LanceDB), Prompt Engineering\n",
      "Content: data into actionable insights and delivering production-ready ML solutions. Competitive ML participant (top \n",
      "30% Zindi), passionate about solving high-impact problems with data. \n",
      "Skills      \n",
      "Programm...\n",
      "\n",
      "Score: 0.483\n",
      "Section: Machine Learning & AI: Classification, Regression, NLP, Deep Learning, Feature Engineering, Model Evaluation\n",
      "Content: Machine Learning & AI: Classification, Regression, NLP, Deep Learning, Feature Engineering, Model Evaluation...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# src/pipeline/query_cv.py\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def query_cv(query, index_name=\"cv-index\", top_k=3):\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    query_vector = model.encode(query).tolist()\n",
    "\n",
    "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "    index = pc.Index(index_name)\n",
    "\n",
    "    results = index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    print(\"üîç Query Results:\")\n",
    "    for match in results[\"matches\"]:\n",
    "        print(f\"Score: {match['score']:.3f}\")\n",
    "        print(f\"Section: {match['metadata']['text']}\")\n",
    "        print(f\"Content: {match['metadata']['text'][:200]}...\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query_cv(\"What are Cleave‚Äôs top machine learning skills?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
